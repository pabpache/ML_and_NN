{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, October 2020\n",
    "\n",
    "- Jeremie Kull - z5208518\n",
    "- Pablo Pacheco - Z5222810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 18th October 2020, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via the [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/20T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make division default to floating-point, saving confusion\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from collections import OrderedDict as odict\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Metastasis\",\"MC\",\"SkinRetract\",\"NippleDischarge\",\"AD\",\"Mass\"],\n",
    "    \"Mass\" : [\"Margin\",\"Shape\",\"Size\"],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [\"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [\"SkinRetract\",\"NippleDischarge\",\"Spiculation\"],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\"],\n",
    "    \"Margin\" : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: true if **X** is d-separated from **Y** given **Z** in the graph $G$ and false otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a graph as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for d_separation(G, X, Z, Y) in one or more cells here\n",
    "\n",
    "#We need to create a recursive function to delete the leaf nodes\n",
    "def delete_leaf_node(G,X,Z,Y):\n",
    "    G_aux=G.copy()\n",
    "    for j in G_aux.keys():\n",
    "        G_aux[j]=G[j].copy()\n",
    "    for k in G_aux.keys():\n",
    "        #check if we are in a leaf node\n",
    "        if not G_aux[k]:\n",
    "            #check if the leaf node is in X,Z or Y\n",
    "            if (k not in X) and (k not in Z) and (k not in Y):\n",
    "                #remove the leaf node\n",
    "                G_aux.pop(k)\n",
    "                #remove the node from the lists of the other nodes\n",
    "                for i in G_aux.keys():\n",
    "                    if k in G_aux[i]:\n",
    "                        G_aux[i].remove(k)\n",
    "                #Repeat the process with the new graph\n",
    "                return delete_leaf_node(G_aux,X,Z,Y)\n",
    "    #There is no more deletable node\n",
    "    return G_aux\n",
    "            \n",
    "#Check connection between nodes using DFS\n",
    "\n",
    "#This function does a DFS and paint grey every visited node, starting from x\n",
    "def dfs_r(G, x, colour):\n",
    "    # Visited vertices are coloured 'grey'\n",
    "    colour[x] = 'grey'\n",
    "    # Let's visit all outgoing edges from x\n",
    "    for w in G[x]:\n",
    "        # To avoid loops, we check if the next vertex hasn't been visited yet\n",
    "        if colour[w] == 'white':\n",
    "            dfs_r(G, w, colour)\n",
    "\n",
    "#The next function returns True when there is a connection between X and Y\n",
    "def check_connection(G,X,Y): \n",
    "    for i in X:\n",
    "        # Create a dictionary with keys as node numbers and values equal to 'white'\n",
    "        colour = {node: 'white' for node in G.keys()}\n",
    "        dfs_r(G,i,colour)\n",
    "        for j in Y:\n",
    "            if colour[j]=='grey':\n",
    "                return True     \n",
    "    return False\n",
    "    \n",
    "def d_separation(G,X,Z,Y):\n",
    "    #we get the new graph with some deleted nodes\n",
    "    G_prime= delete_leaf_node(G,X,Z,Y)\n",
    "  \n",
    "    #Now, we need to delete the edges outgoing from nodes in Z\n",
    "    for k in G_prime.keys():\n",
    "        if k in Z:\n",
    "            G_prime[k]=[]\n",
    "    \n",
    "    #Now, we need to check if X and Y are connected\n",
    "    #Given the directions of the edges don't matter, we need to make an undirected graph\n",
    "    G_undirected=G_prime.copy()\n",
    "    for k in G_undirected.keys():\n",
    "        G_undirected[k]=G_prime[k].copy()\n",
    "    \n",
    "    for k in G_prime.keys():\n",
    "        for j in G_prime[k]:\n",
    "            G_undirected[j].append(k)\n",
    "    \n",
    "    if check_connection(G_undirected,X,Y):\n",
    "        #There is a connection, so X and Y are not d-separated\n",
    "        return False\n",
    "    else:\n",
    "        #There is no a connection, so X and Y are d-separated\n",
    "        return True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','LymphNodes']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a function ``learn_bayes_net(G, data, outcomeSpace)`` that learns the parameters of the Bayesian Network $G$. This function should return a dictionary ``prob_tables`` with the all conditional probability tables (one for each node).\n",
    "\n",
    "- ``G`` is a directed acyclic graph. For this part of the assignment, $G$ should be declared according to the breast cancer Bayesian network presented in the diagram in the assignment specification.\n",
    "- ``data`` is a dataframe created from a csv file containing the relevant data. \n",
    "- ``outcomeSpace`` is defined in tutorials.\n",
    "- ``prob_tables`` is a dict from each variable name (node) to a \"factor\". Factors are defined in tutorial 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here\n",
    "\n",
    "def learn_outcome_space(data):\n",
    "    outcomeSpace=dict()\n",
    "    for i in data.columns:\n",
    "        outcomeSpace[i]=tuple(data[i].unique())\n",
    "    return outcomeSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_bayes_net(G, data, outcomeSpace) in one or more cells here\n",
    "#The first three function are the same as what we saw in the tutorial 3. \n",
    "\n",
    "#This function is used to get the parents of very node in order to get the conditional probabilities.\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v, []) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return GT\n",
    "\n",
    "#This function is used to know where the given variables or parents or fixed_variables are happening.\n",
    "def allEqualThisIndex(dict_of_arrays, **fixed_vars):\n",
    "    first_array = dict_of_arrays[list(dict_of_arrays.keys())[0]]\n",
    "    index = np.ones_like(first_array, dtype=np.bool_)\n",
    "    for var_name, var_val in fixed_vars.items():\n",
    "        index = index & (np.asarray(dict_of_arrays[var_name])==var_val)\n",
    "    return index\n",
    "\n",
    "#The next function gives the method to calculate the probability tables. It is going to be used by\n",
    "# learn_bayes_net function\n",
    "def estProbTable(data, var_name, parent_names, outcomeSpace): \n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in (parent_names)]\n",
    "    # cartesian product to generate a table of all possible outcomes\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "\n",
    "    prob_table = odict()\n",
    "    \n",
    "    # Smoothing\n",
    "    alpha = 1\n",
    "    \n",
    "    for i, parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names, parent_combination))\n",
    "        parent_index = allEqualThisIndex(data, **parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            prob_table[tuple(list(parent_combination)+[var_outcome])] = ((var_index & parent_index).sum() + alpha) / (parent_index.sum() + alpha * len(var_outcomes))\n",
    "            \n",
    "    return {'dom': tuple(list(parent_names)+[var_name]), 'table': prob_table}\n",
    "\n",
    "#The next function return the probability tables of each node, considering the given graph structure\n",
    "def learn_bayes_net(G, data, outcomeSpace):\n",
    "    prob_tables = odict()\n",
    "    GT=transposeGraph(G)\n",
    "    for node, parents in GT.items():\n",
    "        prob_tables[node] = estProbTable(data,node,parents,outcomeSpace)\n",
    "    return prob_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "test(abs(prob_tables['Age']['table'][('35-49',)] - 0.2476) < 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)`` that uses the test cases in ``data`` to assess the performance of the Bayesian network defined by ``G`` and ``prob_tables``. Implement the efficient classification procedure discussed in the lectures. Such a function should return the classifier accuracy. \n",
    " * ``class_var`` is the name of the variable you are predicting, using all other variables.\n",
    " * ``outcomeSpace`` was created in task 2\n",
    " \n",
    "Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Return just the accuracy:\n",
    "\n",
    "``acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "#Remove metastasis and lymphnodes from graph and dataset:\n",
    "G = {\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"MC\",\"SkinRetract\",\"NippleDischarge\",\"AD\",\"Mass\"],\n",
    "    \"Mass\" : [\"Margin\",\"Shape\",\"Size\"],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [\"SkinRetract\",\"NippleDischarge\",\"Spiculation\"],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\"],\n",
    "    \"Margin\" : [],\n",
    "}\n",
    "\n",
    "data.drop(columns=['Metastasis','LymphNodes'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Blanket of a variable class_var in a graph G. This returns a list with the nodes in the blanket\n",
    "def blanket(G,class_var):\n",
    "    blanket_list=list()\n",
    "    #Children\n",
    "    for i in G[class_var]:\n",
    "        blanket_list.append(i)\n",
    "    \n",
    "    for k in G.keys():\n",
    "        if k != class_var:\n",
    "            #Spouses\n",
    "            for child in G[k]:\n",
    "                if child in G[class_var] and k not in blanket_list:\n",
    "                    blanket_list.append(k)\n",
    "                    break\n",
    "            #Parents\n",
    "            if class_var in G[k] and k not in blanket_list:\n",
    "                blanket_list.append(k)\n",
    "    return blanket_list\n",
    "\n",
    "#function from tutorial to calculate probability given an entry\n",
    "def prob(factor, *entry):\n",
    "    return factor['table'][entry]  \n",
    "\n",
    "#function from tutoral to calculate join probability given two factors\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    \n",
    "    # We will build a table from scratch, starting with an empty list. \n",
    "    table = list()\n",
    "    \n",
    "    # The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = (entryDict[var] for var in f1['dom'])\n",
    "        f2_entry = (entryDict[var] for var in f2['dom'])\n",
    "        \n",
    "        p1 = prob(f1, *f1_entry)           \n",
    "        p2 = prob(f2, *f2_entry)        \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "#Function from tutorial to get a new outcomespace given some evidence\n",
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      \n",
    "    newOutcomeSpace[var] = (e,)             \n",
    "    return newOutcomeSpace\n",
    "\n",
    "#Function from tutorial to marginalize a variable from a factor\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    \n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). \n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. \n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. \n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "                      \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. \n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s.\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "\n",
    "#Function from tutorial to make queries, the only difference is that this function does not normalize at \n",
    "#the end. Given that we are classifying, it is just needed to choose the most likely one. In addition, we are\n",
    "#assuming that q_vars is going to be just one variable because we are doing classification.\n",
    "def query(p, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `p`, probability table to query.\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor  with all hidden variables eliminated as evidence set as in q_evi\n",
    "    \"\"\"     \n",
    "    \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    pm = p.copy()\n",
    "    outSpace = outcomeSpace.copy()\n",
    "    \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)\n",
    "        \n",
    "    # Second, we eliminate hidden variables NOT in the query\n",
    "    for var in outSpace:\n",
    "        if var != q_vars:                         #this is different to the tutorial, we are assuming just one var\n",
    "            pm = marginalize(pm, var, outSpace)\n",
    "    return pm\n",
    "\n",
    "\n",
    "def assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var):\n",
    "    \n",
    "    # We are going to use just the Markovian Blanket as evidence, instead of using all variables\n",
    "    blanket_list= blanket(G,class_var)\n",
    "    \n",
    "    # blanket_indexes\n",
    "    blanket_dict=dict()\n",
    "    for b in blanket_list:\n",
    "        blanket_dict[b]=data.columns.get_loc(b)\n",
    "    # class_var index\n",
    "    class_var_index=data.columns.get_loc(class_var)\n",
    "    \n",
    "    def n_cons(e):\n",
    "        cons = len(G[e])\n",
    "        for i in G.keys():\n",
    "            if e in G[i]: cons += 1\n",
    "        return cons\n",
    "    \n",
    "    # print(blanket_list)\n",
    "    \n",
    "    # Using MinDegreeOrder talked about in lectures significantly improves performance for TAN probability table joining\n",
    "    blanket_list.sort(reverse=True, key=n_cons)\n",
    "\n",
    "    # we need to compute the full joint probability of the Bayesian network (using joint function)\n",
    "    p_join=prob_tables[class_var]\n",
    "    for k in blanket_list:\n",
    "        p_join = join(p_join, prob_tables[k], outcomeSpace)\n",
    "        \n",
    "    # we need a new outcomeSpace with only the variables in the blanket and its parents\n",
    "    # Given that we have complete data, we just need the joint probability of the \n",
    "    # markovian balnket of the class_var because of the independencies\n",
    "    outcomeSpace_join=dict()\n",
    "    for k in p_join['dom']:\n",
    "        outcomeSpace_join[k]=outcomeSpace[k]\n",
    "    \n",
    "    n_coincidence=0\n",
    "    n_rows=len(data.index)\n",
    "    \n",
    "    \n",
    "    # working with numpy for more efficiency\n",
    "    #data2=data.to_numpy()    #This could be the best option, but CSE servers don't have the requiered version of pandas\n",
    "    data2=data.values\n",
    "    # Now, we can make queries. We are going to use query function, which uses marginalization\n",
    "    for row in data2:\n",
    "        # create dictionary with the given variables in blanket\n",
    "        evidence=dict()\n",
    "        for b in blanket_list:\n",
    "            evidence[b]=row[blanket_dict[b]]\n",
    "        # make the query\n",
    "        proba=query(p_join,outcomeSpace_join,class_var,**evidence)\n",
    "        # check the most likely\n",
    "        max_prob = -1\n",
    "        classification=''\n",
    "        for i in proba['table'].keys():\n",
    "            if proba['table'][i] > max_prob:\n",
    "                max_prob=proba['table'][i]\n",
    "                classification=i[0]\n",
    "        # Check if it was correct classified\n",
    "        if classification==row[class_var_index]:\n",
    "            n_coincidence +=1\n",
    "\n",
    "    return n_coincidence/n_rows\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomeSpace and prob_tables considering the 2 removed variables\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "prob_tables = learn_bayes_net(G, data, outcomeSpace)\n",
    "#Defining variable to classify\n",
    "class_var='BC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.612837791442871\n",
      "0.8423\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a function ``cv_bayes_net(G, data, class_var)`` that uses ``learn_outcome_space``, ``learn_bayes_net``and ``assess_bayes_net`` to learn and assess a Bayesian network in a dataset using 10-fold cross-validation. Compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation, e.g.\n",
    "\n",
    "``acc, stddev = cv_bayes_net(G, data, class_var)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_bayes_net(G, data, class_var) in one or more cells here\n",
    "\n",
    "def cv_bayes_net(G, data, class_var):\n",
    "    #Getting the outcomeSpace. It is obtained from the all data, because I know what are the all possibilities\n",
    "    #that I'm going to classify. What could happened is that we don't see any of those combinations in the training data\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    \n",
    "    #we need to split the data in k=10\n",
    "    n_folds=10\n",
    "    acc_array=np.ones(n_folds)\n",
    "    #data2=data.to_numpy()\n",
    "    data2=data.values\n",
    "    k_folds= np.array_split(data2,n_folds)\n",
    "    \n",
    "    for k in range(n_folds):\n",
    "        test_data=pd.DataFrame(k_folds[k],columns=data.keys())\n",
    "        \n",
    "        if k==0:\n",
    "            training=k_folds[k+1]\n",
    "            for j in range(k+2,n_folds):\n",
    "                training= np.concatenate((training,k_folds[j]),axis=0)\n",
    "        else:\n",
    "            training=k_folds[0]\n",
    "            for j in range(1,n_folds):\n",
    "                if j!=k:\n",
    "                    training=np.concatenate((training,k_folds[j]),axis=0)\n",
    "        \n",
    "        training_data = pd.DataFrame(training,columns=data.keys())\n",
    "        \n",
    "        \n",
    "        #learn the bayesian network from training data\n",
    "        prob_tables=learn_bayes_net(G, training_data, outcomeSpace)\n",
    "        \n",
    "        #We have a model, now we need to evaluate it (using test data)\n",
    "        acc_array[k]= assess_bayes_net(G, prob_tables, test_data, outcomeSpace, class_var)\n",
    "      \n",
    "            \n",
    "    #we are going to use population standard deviation of numpy     \n",
    "    #print('array:',acc_array)    \n",
    "    return (np.average(acc_array),np.std(acc_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.345041275024414\n",
      "0.8412000000000001 0.00580603134679791\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "start=time.time()\n",
    "acc, stddev = cv_bayes_net(G, data, 'BC')\n",
    "end=time.time()\n",
    "print('time:',end-start)\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Naïve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to classify and assess the test cases in a dataset ``data`` according to the Naïve Bayes classifier. To classify each example, use the log probability trick discussed in the lectures. This function should return the accuracy of the classifier in ``data``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var) in one or more cells here\n",
    "\n",
    "def assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var):    \n",
    "    #We are ASSUMMING that G HAS THE NAIVE BAYES STRUCTURE\n",
    "    #And we are assumming complete data, so every node in the graph is going to have a column in data\n",
    "   \n",
    "\n",
    "    #evidence_indexes\n",
    "    evi_index=dict()\n",
    "    for i in G.keys():\n",
    "        if i!=class_var:\n",
    "            evi_index[i]=data.columns.get_loc(i)\n",
    "    #class_var index\n",
    "    class_var_index=data.columns.get_loc(class_var)\n",
    "  \n",
    "    n_coincidence=0\n",
    "    n_rows=len(data.index)\n",
    "    \n",
    "    #data2=data.to_numpy()\n",
    "    data2=data.values\n",
    "    \n",
    "    for row in data2:\n",
    "        #create dictionary with the given variables\n",
    "        evidence=dict()\n",
    "        for i in evi_index.keys():\n",
    "            evidence[i]=row[evi_index[i]]\n",
    "                   \n",
    "        max_prob=-1000000000000000\n",
    "        classification=''\n",
    "        #Given naive bayes assumption, we can use directly the conditional probability tables\n",
    "        #calculate probability for each value of class_var using naive bayes assumption\n",
    "        for v in outcomeSpace[class_var]:\n",
    "            #the first component of the joint probability is P(class_var). We are going to use the log 'trick'\n",
    "            if prob_tables[class_var]['table'][(v,)] > 0:\n",
    "                prob_v= math.log(prob_tables[class_var]['table'][(v,)])\n",
    "            else:\n",
    "                prob_v=-1000000\n",
    "            #Now, we compute the rest of the probability. So, we need to sum (we are using log) the probability\n",
    "            #of every evidence given class_var\n",
    "            for e in evidence:\n",
    "                if prob_tables[e]['table'][(v,evidence[e])] > 0:\n",
    "                    prob_v = prob_v + math.log(prob_tables[e]['table'][(v,evidence[e])])\n",
    "                else:\n",
    "                    prob_v = prob_v -1000000\n",
    "            if prob_v > max_prob:\n",
    "                classification=v\n",
    "                max_prob=prob_v\n",
    "            \n",
    "        #Check if it was correct classified\n",
    "        if classification==row[class_var_index]:\n",
    "            n_coincidence +=1\n",
    "    \n",
    "    return n_coincidence/n_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "#THIS JUST WORK WHEN YOU GIVE A GRAPH G WITH A NAIVE STRUCTURE\n",
    "#prob_tables=learn_bayes_net(naive_graph, data, outcomeSpace)\n",
    "#acc = assess_naive_bayes(naive_graph, prob_tables, data, outcomeSpace, 'BC')\n",
    "#print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a new function ``cv_naive_bayes(data, class_var)`` that uses ``assess_naive_bayes`` to assess the performance of the Naïve Bayes classifier in a dataset ``data``. To develop this code, perform the following steps:\n",
    "\n",
    "1. Use 10-fold cross-validation to split the data into training and test sets.\n",
    "\n",
    "2. Implement a function ``learn_naive_bayes_structure(outcomeSpace, class_var)`` to create and return a Naïve Bayes graph structure from ``outcomeSpace`` and ``class_var``. \n",
    "\n",
    "3. Use ``learn_bayes_net(G, data, outcomeSpace)`` to learn the Naïve Bayes parameters from a training set ``data``. \n",
    "\n",
    "4. Use ``assess_naive_bayes(G, prob_tables, data, outcomeSpace, class_var)`` to compute the accuracy of the Naïve Bayes classifier in a test set ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy.\n",
    "\n",
    "Do 10-fold cross-validation, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_naive_bayes_structure(outcomeSpace, class_var) in one or more cells here\n",
    "#This function makes the class_var the oly parent of all other variables (nodes)\n",
    "def learn_naive_bayes_structure(outcomeSpace,class_var): \n",
    "    new_G=dict()\n",
    "    for k in outcomeSpace.keys():\n",
    "        if k==class_var:\n",
    "            new_G[k]=list(outcomeSpace.keys())\n",
    "            new_G[k].remove(k)\n",
    "        else:\n",
    "            new_G[k]=[]\n",
    "    return new_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "naive_graph = learn_naive_bayes_structure(outcomeSpace, 'BC')\n",
    "prueba=learn_bayes_net(naive_graph,data,outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_naive_bayes(data, class_var) in one or more cells here\n",
    "\n",
    "def cv_naive_bayes(data, class_var):\n",
    "    # get the outcomeSpace. It is obtained from the all data, because we know what are the all possibilities\n",
    "    # that we going to classify. What could happened is that we don't see any of those combinations in the training data\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    \n",
    "    G = learn_naive_bayes_structure(outcomeSpace, class_var)\n",
    "    \n",
    "    # we need to split the data in k=10\n",
    "    n_folds=10\n",
    "    acc_array=np.ones(n_folds)\n",
    "    #data2=data.to_numpy()\n",
    "    data2=data.values\n",
    "    k_folds= np.array_split(data2,n_folds)\n",
    "    \n",
    "    for k in range(n_folds):\n",
    "        test_data=pd.DataFrame(k_folds[k],columns=data.keys())\n",
    "        \n",
    "        if k==0:\n",
    "            training=k_folds[k+1]\n",
    "            for j in range(k+2,n_folds):\n",
    "                training= np.concatenate((training,k_folds[j]),axis=0)\n",
    "        else:\n",
    "            training=k_folds[0]\n",
    "            for j in range(1,n_folds):\n",
    "                if j!=k:\n",
    "                    training=np.concatenate((training,k_folds[j]),axis=0)\n",
    "        \n",
    "        training_data = pd.DataFrame(training,columns=data.keys())\n",
    "        \n",
    "        \n",
    "        # learn the bayesian network from training data\n",
    "        prob_tables=learn_bayes_net(G, training_data, outcomeSpace)\n",
    "        \n",
    "        # We have a model, now we need to evaluate it (using test data)\n",
    "        acc_array[k]= assess_naive_bayes(G, prob_tables, test_data, outcomeSpace, class_var)\n",
    "        \n",
    "        # Unoptimised general version:\n",
    "        #acc_array[k]= assess_bayes_net(G, prob_tables, test_data, outcomeSpace, class_var)\n",
    "      \n",
    "            \n",
    "    # we are going to use population standard deviation of numpy     \n",
    "    # print('array:',acc_array)    \n",
    "    return (np.average(acc_array),np.std(acc_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0318539142608643\n",
      "0.79185 0.008697844560579362\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "start=time.time()\n",
    "acc, stddev = cv_naive_bayes(data, 'BC')\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Naïve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Naïve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, outcomeSpace, class_var)`` to learn the TAN structure (graph) from the ``data`` and returns such a structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prim(G, m, vals):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `s`, start vertex\n",
    "    \"\"\"      \n",
    "    s = vals[0]\n",
    "    # Intialise set S with vertex s\n",
    "    S = {s}\n",
    "    # Initialise priority queue Q with an empty list\n",
    "    Q = []\n",
    "    # Initilise list tree with empty list. This variable will have the MST at the end of the execution\n",
    "    tree = []\n",
    "    # Initilise the priority queue Q with outgoing edges from s\n",
    "    for e in G[s]:\n",
    "        # There is a trick here. Python prioriy queues accept tuples but the first entry of the tuple must be the priority value\n",
    "        pq.heappush(Q, [-m[vals.index(s)][vals.index(e)], s, e])\n",
    "    while len(Q) > 0:\n",
    "        # Remove element from Q with the smallest weight\n",
    "        [cost, v, u] = pq.heappop(Q)\n",
    "        # If the node is already in S we cannot include it in the MST since it would create a cycle\n",
    "        if not u in S:\n",
    "            # Let's grow the MST by inserting the vertex in S\n",
    "            S.add(u)\n",
    "            # Also we insert the edge in tree, use v, u, cost order\n",
    "            tree.append([v, u, cost])\n",
    "            # We iterate over all outgoing edges of u[1] (or \"v\" according to the algorithm)\n",
    "            for e in G[u]:\n",
    "                # We are interested in edges that connect to vertices not is S and with smaller weight than known values stores in a\n",
    "                if not e in S:\n",
    "                    # Edge e is of interest, let's store in the priority queue for future analysis\n",
    "                    pq.heappush(Q, [-m[vals.index(u)][vals.index(e)], u, e])        \n",
    "    return tree\n",
    "        \n",
    "#######################################\n",
    "# Test code\n",
    "#######################################\n",
    "\n",
    "## Develop your code for learn_tan_structure(data, outcomeSpace, class_var) in one or more cells here\n",
    "def learn_tan_structure(data, outcomeSpace, class_var):\n",
    "    new_G = dict()\n",
    "    vals = list(outcomeSpace.keys())\n",
    "    c = vals.index(class_var)\n",
    "    vals.remove(class_var)\n",
    "\n",
    "    m = [[0 for i in range(len(vals))] for i in range(len(vals))]\n",
    "\n",
    "    # print(outcomeSpace)\n",
    "    # print(class_var)\n",
    "    # print(\"#####################\")\n",
    "    # print(data)\n",
    "    \n",
    "    #data2 = data.to_numpy()\n",
    "    data2=data.values\n",
    "\n",
    "    for k in range(len(vals)):\n",
    "        if (k == c): continue\n",
    "        for j in range(k + 1, len(vals)):\n",
    "            v1 = vals[k]\n",
    "            v2 = vals[j]\n",
    "            \n",
    "            # Since we skip the c'th collumn we need to do this\n",
    "            data_k = k\n",
    "            data_j = j\n",
    "            if (k > c):\n",
    "                data_k += 1\n",
    "            if (j > c):\n",
    "                data_j += 1\n",
    "\n",
    "            M = 0\n",
    "            for o3 in outcomeSpace[class_var]:\n",
    "                c_true = (data2[:,c] == o3)\n",
    "                count_c = np.count_nonzero(c_true)\n",
    "                for o1 in outcomeSpace[v1]:\n",
    "                    v1_true = (data2[:,data_k] == o1)\n",
    "                    for o2 in outcomeSpace[v2]:\n",
    "                        v2_true = (data2[:,data_j] == o2)\n",
    "                        # use columns\n",
    "                        count_v1_v2_c = np.count_nonzero(v1_true & v2_true & c_true)\n",
    "                        count_v1_c = np.count_nonzero(v1_true & c_true)\n",
    "                        count_v2_c = np.count_nonzero(v2_true & c_true)\n",
    "                        \n",
    "                        # Using smoothing                        \n",
    "                        alpha = 1\n",
    "                        \n",
    "                        # P(v1 = o1, v2 = o2 | c = o3)\n",
    "                        P_v1_v2_lc = (count_v1_v2_c + alpha) / (count_c + alpha * len(outcomeSpace[v1]) * len(outcomeSpace[v2]))\n",
    "                        # P(v1 = o1 | c = o3)\n",
    "                        P_v1_lc = (count_v1_c + alpha) / (count_c + alpha * len(outcomeSpace[v1]))\n",
    "                        # P(v2 = o1 | c = o3)\n",
    "                        P_v2_lc = (count_v2_c + alpha) / (count_c + alpha * len(outcomeSpace[v2]))\n",
    "                        # P(v1 = o1, v2 = o2, c = o3)\n",
    "                        n_vals = len(outcomeSpace[class_var]) * len(outcomeSpace[v1]) * len(outcomeSpace[v2])\n",
    "                        P_v1_v2_c = (count_v1_v2_c + alpha) / (data2.shape[0] + alpha * n_vals)\n",
    "                        \n",
    "                        \n",
    "                        # Just in case (shouldn't need it due to smoothing)\n",
    "                        if (not P_v1_v2_lc): P_v1_v2_lc = 0.000001\n",
    "                        if (not P_v1_lc): P_v1_lc = 0.000001\n",
    "                        if (not P_v2_lc): P_v2_lc = 0.000001\n",
    "                        \n",
    "                        # print(P_v1_v2_lc, (P_v1_lc * P_v2_lc), P_v1_v2_lc / (P_v1_lc * P_v2_lc))\n",
    "\n",
    "                        M += P_v1_v2_c * math.log(P_v1_v2_lc / (P_v1_lc * P_v2_lc))\n",
    "            \n",
    "            m[k][j] = M\n",
    "            m[j][k] = M\n",
    "\n",
    "    # [[print(int(i*100), end=' ') for i in l] and print() for l in m]\n",
    "    \n",
    "    G = {}\n",
    "    for i in vals: G[i] = [j for j in vals]\n",
    "    # print(G)\n",
    "    # print()\n",
    "    max_span = prim(G, m, vals)\n",
    "    \n",
    "    # TODO # make directed tree\n",
    "    \n",
    "    n_m = {}\n",
    "    n_m[class_var] = vals\n",
    "    # n_m[tmp[0][0]] = [tmp[0][1]]\n",
    "    # added = [tmp[0][0]]\n",
    "    # del tmp[0]\n",
    "    \n",
    "    queue = [max_span[0][0]]\n",
    "    # queue <- max_span[0]\n",
    "    # pop(), then find all edges that connect to it, add the other end to the queue, remove edge from max_span\n",
    "    # until all vertex are added\n",
    "    \n",
    "    # Both directions\n",
    "    while len(queue):\n",
    "        val = queue.pop(0)\n",
    "        n_m[val] = []\n",
    "        i = 0\n",
    "        while i < len(max_span):\n",
    "            if max_span[i][0] == val:\n",
    "                n_m[val] += [max_span[i][1]]\n",
    "                queue.append(max_span[i][1])\n",
    "                max_span.remove(max_span[i])\n",
    "            elif max_span[i][1] == val:\n",
    "                n_m[val] += [max_span[i][0]]\n",
    "                queue.append(max_span[i][0])\n",
    "                max_span.remove(max_span[i])\n",
    "            else:\n",
    "                i += 1\n",
    "#         if v[0] in n_m.keys(): n_m[v[0]] += [v[1]]\n",
    "#         else:  n_m[v[0]] = [v[1]]\n",
    "        \n",
    "#         if v[1] in n_m.keys(): n_m[v[1]] += [v[0]]\n",
    "#         else:  n_m[v[1]] = [v[0]]    \n",
    "    \n",
    "    # print(n_m)\n",
    "    \n",
    "    return n_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1415932178497314\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "start=time.time()\n",
    "\n",
    "tan_graph = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "#print(tan_graph)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "test(len(tan_graph['BC']) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph['Spiculation'] or 'Spiculation' in tan_graph['FibrTissueDev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the other tasks, design a function ``cv_tan(data, class_var)`` that uses 10-fold cross-validation to assess the performance of the TAN classifier from ``data``. Remember to remove the variables ``metastasis`` and ``lymphnodes`` from the dataset before assessing the accuracy. This function should use the ``learn_tan_structure`` as well as other functions defined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for cv_tan(data, class_var) in one or more cells here\n",
    "def cv_tan(data, class_var):\n",
    "    #get the outcomeSpace. It is obtained from the all data, because we know what are the all possibilities\n",
    "    #that we going to classify. What could happened is that we don't see any of those combinations in the training data\n",
    "    outcomeSpace = learn_outcome_space(data)\n",
    "    \n",
    "    # G = learn_naive_bayes_structure(outcomeSpace,class_var)\n",
    "    # print(G)\n",
    "    # print(\"--------------\")\n",
    "    \n",
    "    # print(G)\n",
    "    \n",
    "    #we need to split the data in k=10\n",
    "    n_folds=10\n",
    "    acc_array=np.ones(n_folds)\n",
    "    #data2=data.to_numpy()\n",
    "    data2=data.values\n",
    "    k_folds= np.array_split(data2,n_folds)\n",
    "    \n",
    "    for k in range(n_folds):\n",
    "        test_data=pd.DataFrame(k_folds[k], columns=data.keys())\n",
    "        \n",
    "        if k==0:\n",
    "            training=k_folds[k+1]\n",
    "            for j in range(k+2,n_folds):\n",
    "                training= np.concatenate((training, k_folds[j]),axis=0)\n",
    "        else:\n",
    "            training=k_folds[0]\n",
    "            for j in range(1,n_folds):\n",
    "                if j!=k:\n",
    "                    training=np.concatenate((training, k_folds[j]),axis=0)\n",
    "        \n",
    "        training_data = pd.DataFrame(training,columns=data.keys())\n",
    "        \n",
    "        G = learn_tan_structure(training_data, outcomeSpace, class_var)\n",
    "        \n",
    "        #learn the bayesian network from training data\n",
    "        prob_tables = learn_bayes_net(G, training_data, outcomeSpace)\n",
    "        \n",
    "        #We have a model, now we need to evaluate it (using test data)\n",
    "        acc_array[k]= assess_bayes_net(G, prob_tables, test_data, outcomeSpace, class_var)\n",
    "      \n",
    "            \n",
    "    #we are going to use population standard deviation of numpy     \n",
    "    #print('array:', acc_array)    \n",
    "    return (np.average(acc_array), np.std(acc_array))\n",
    "\n",
    "\n",
    "# outcomeSpace = learn_outcome_space(data)\n",
    "# G = learn_tan_structure(data, outcomeSpace, class_var)\n",
    "# print(\"start\")\n",
    "# start=time.time()\n",
    "# acc = assess_bayes_net(G, prob_tables, data, outcomeSpace, class_var)\n",
    "# print(time.time()-start)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.32281923294067\n",
      "0.8174499999999998 0.005815711478400541\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "start=time.time()\n",
    "acc, stddev = cv_tan(data, 'BC')\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "print(acc,stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results (accuracy). Use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your report in one or more cells here\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are mostly consistent with what we expected, the first Bayes Network (GBN) runs the quickly (more details on speed later), and performs the best, at 0.8412 accuracy with 0.0058 standard deviation. The Naive Bayes (NB) runs slower (without using ‘assess_naive_bayes’), and has worse performance, at 0.7919 accuracy and 0.0087 SD, but can be optimised to run extremely fast using the NB assumption. Finally, the Tree Augmented Naive Bayes (TAN) runs a bit slower than NB, but performs better, however still worse than the BGN with 0.8174 accuracy and 0.0058 SD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = ['GBN', 'NB', 'NB (optimised)', 'TAN']\n",
    "energy = [0.8412, 0.7919, 0.7919, 0.8174]\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "barlist = ax1.bar(x_pos, energy)\n",
    "barlist[0].set_color('red')\n",
    "barlist[1].set_color('green')\n",
    "barlist[2].set_color('blue')\n",
    "barlist[3].set_color('purple')\n",
    "ax1.set_ylabel(\"Success probaility [0-1]\")\n",
    "ax1.set_title(\"Accuracy\")\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(x)\n",
    "\n",
    "\n",
    "x = ['GBN', 'NB', 'NB (optimised)', 'TAN']\n",
    "energy = [7.172, 83.16, 1.828, 93.01]\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "barlist = ax2.bar(x_pos, energy)\n",
    "barlist[0].set_color('red')\n",
    "barlist[1].set_color('green')\n",
    "barlist[2].set_color('blue')\n",
    "barlist[3].set_color('purple')\n",
    "ax2.set_ylabel(\"Time taken (seconds)\")\n",
    "ax2.set_title(\"Time\")\n",
    "\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, as our original GBN has the exact causal network given to it, whereas NB makes the generally incorrect assumption that all variables are independent. TAN figures out some dependency information from the data, so improves vs NB, but still relies on the data rather than the true causal links between variables, and thus performs worse than GBN. Referring to this (https://link.springer.com/chapter/10.1007%2F978-1-84882-171-2_1) paper on the topic of performance of GBN, NB and TAN’s, we see that this mostly supports our findings, as GBN should significantly outperform NB, and TAN should be better than NB. The only discrepancy is that the paper shows that GBN and TAN should be similar, whereas our data shows TAN performing worse. This is likely due to our GBN being constructed from ground truth biological causality, whereas they used heuristically generated GBN’s in the paper. \n",
    "\n",
    "The complexity of ‘assess_bayes_net’ is mostly dependent on the creation of the full joint probability network, which uses the ‘join’ function to combine multiple probability tables into a larger one. The complexity of this is roughly $O(|X|^n)$, where $|X|$ is the maximum outcome space size, and $n$ is the number of unique variables in the probability tables to be combined. Since this is exponential in the number of variables, using a Markov blanket to remove unnecessary variables greatly improves performance. Additionally, using MinDegreeOrder shown in lectures for variable elimination greatly helps speed up the calculation for the TAN. Despite these optimisations, this still takes a long time if the Markov blanket cannot reduce the number of variables involved, such as for the NB and TAN. We also made a super-fast version of ‘assess_naive_bayes’ that uses the fact that we don’t need to compute the full combined probability table for it, we can simply multiply the independent probabilities together.\n",
    "\n",
    "Once the conditional probability table is made, querying the table for all data is $O(v n)$  where $v$ is the number of values in the dataset. This is very fast as it scales linearly with more data.\n",
    "\n",
    "The creation of the TAN is $O(n^2 |X|^3 v^3)$, since the outcome spaces are fairly small, $|X|$ can be ignored, and the $v^3$ component is done using NumPy vectorised operations which are super fast compared to normal python loops and thus the TAN can be generated in about 1 second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
